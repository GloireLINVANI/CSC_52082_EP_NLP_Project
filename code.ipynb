{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nawel/Library/Python/3.13/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m715.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m382.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.3 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.10.6)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Downloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m400.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, jiter, h11, distro, httpcore, anyio, httpx, openai\n",
      "Successfully installed anyio-4.8.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.66.3 sniffio-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Différents tests </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> l'idée c'est de faire un code qui après le dernier poitn regarde ce qu'il ya apres, s'il n'y a qu'un mot on le remove, s'il y'a une phrase on la complete. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement des ressources NLTK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Télécharger explicitement toutes les ressources NLTK nécessaires\n",
    "print(\"Téléchargement des ressources NLTK...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab', quiet=True, raise_on_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting psutil (from accelerate)\n",
      "  Downloading psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/nawel/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Collecting jinja2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /Users/nawel/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=2.0.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nawel/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->accelerate)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.5.1-py3-none-any.whl (345 kB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m54.5/66.5 MB\u001b[0m \u001b[31m41.6 kB/s\u001b[0m eta \u001b[36m0:04:49\u001b[0mm\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/commands/install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/Users/nawel/miniforge3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installer les paquets nécessaires\n",
    "!pip install transformers sentencepiece accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer et configurer dans une nouvelle cellule\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fonction pour compléter avec un modèle Hugging Face\n",
    "def complete_with_transformers(incomplete_text):\n",
    "    try:\n",
    "        # Charge un petit modèle qui fonctionne sur CPU\n",
    "        if not hasattr(complete_with_transformers, \"generator\"):\n",
    "            print(\"Chargement du modèle (première fois seulement)...\")\n",
    "            complete_with_transformers.generator = pipeline('text-generation', \n",
    "                                                          model='distilgpt2',\n",
    "                                                          device=-1)  # CPU\n",
    "        \n",
    "        # Générer la complétion\n",
    "        result = complete_with_transformers.generator(incomplete_text, \n",
    "                                                    max_length=len(incomplete_text.split()) + 15,\n",
    "                                                    num_return_sequences=1)[0]['generated_text']\n",
    "        \n",
    "        # Retourner uniquement la partie complétée\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la complétion avec Transformers: {e}\")\n",
    "        return incomplete_text + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(60000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# En haut du notebook\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mémoire utilisée: 111.08 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def check_memory():\n",
    "    print(f\"Mémoire utilisée: {psutil.Process().memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Appelez périodiquement dans votre boucle\n",
    "if i % 100 == 0:\n",
    "    check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture du fichier subset.json...\n",
      "Traitement de 3 éléments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des résumés: 100%|██████████| 3/3 [00:00<00:00, 587.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complétion de la phrase: 'Les facteurs de risque incluent le tabagisme, l'alcoolisme, les régimes alimentaires, les infections, et les polluants industriels reconnus cancérigènes'\n",
      "Erreur lors de la complétion avec Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x169602270>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Suppression du mot 'Réponse:' en fin de résumé\n",
      "Complétion de la phrase: 'Les professionnels de la santé recommandent des rapports protég'\n",
      "Erreur lors de la complétion avec Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x16aaa9d10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "Écriture du résultat dans result.json...\n",
      "\n",
      "Traitement terminé en 0.01 secondes\n",
      "Statistiques:\n",
      "- Résumés modifiés: 3/3\n",
      "- Mots uniques supprimés: 1\n",
      "- Phrases complétées: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm  # Version standard (pas .notebook)\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "input_file = \"subset.json\"\n",
    "output_file = \"result.json\"\n",
    "batch_size = 10  # Réduit pour sauvegarder plus fréquemment\n",
    "\n",
    "# Fonction pour compter les mots\n",
    "def count_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "# Fonction pour compléter avec Ollama (modèle local)\n",
    "def complete_with_ollama(incomplete_text):\n",
    "    try:\n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json={\n",
    "                                   'model': 'mistral:7b-instruct-v0.2-q4_0',\n",
    "                                   'prompt': f\"Complete the following sentence in a coherent and concise manner: '{incomplete_text}'\",\n",
    "                                   'stream': False\n",
    "                               }, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', incomplete_text + \".\")\n",
    "        else:\n",
    "            print(f\"Erreur Ollama: {response.text}\")\n",
    "            return incomplete_text + \".\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la complétion avec Ollama: {e}\")\n",
    "        return incomplete_text + \".\"\n",
    "\n",
    "# Fonction pour traiter chaque résumé\n",
    "def process_summary(summary):\n",
    "    if not summary or \".\" not in summary:\n",
    "        return summary\n",
    "    \n",
    "    try:\n",
    "        main_text, last_part = summary.rsplit(\".\", 1)\n",
    "        last_part = last_part.strip()\n",
    "        \n",
    "        if not last_part:\n",
    "            return main_text + \".\"\n",
    "        \n",
    "        word_count = count_words(last_part)\n",
    "        \n",
    "        if word_count == 1:\n",
    "            print(f\"Suppression du mot '{last_part}' en fin de résumé\")\n",
    "            return main_text + \".\"\n",
    "        \n",
    "        elif word_count > 1:\n",
    "            print(f\"Complétion de la phrase: '{last_part}'\")\n",
    "            completed_sentence = complete_with_ollama(last_part)\n",
    "            return main_text + \". \" + completed_sentence\n",
    "        \n",
    "        else:\n",
    "            return main_text + \".\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement du résumé: {e}\")\n",
    "        return summary\n",
    "\n",
    "# Fonction pour sauvegarder la progression\n",
    "def save_progress(current_index):\n",
    "    with open(\"progress.txt\", \"w\") as f:\n",
    "        f.write(str(current_index))\n",
    "\n",
    "# Fonction pour charger la progression\n",
    "def load_progress():\n",
    "    try:\n",
    "        with open(\"progress.txt\", \"r\") as f:\n",
    "            return int(f.read().strip())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Fonction principale - pour meilleure organisation\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Lire le fichier JSON\n",
    "        print(f\"Lecture du fichier {input_file}...\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            items = data\n",
    "        else:\n",
    "            items = [data]\n",
    "        \n",
    "        total_items = len(items)\n",
    "        print(f\"Traitement de {total_items} éléments...\")\n",
    "        \n",
    "        # Variables pour les statistiques\n",
    "        modified_count = 0\n",
    "        removed_words = 0\n",
    "        completed_phrases = 0\n",
    "        errors = []\n",
    "        \n",
    "        # Reprendre le traitement si interrompu précédemment\n",
    "        start_index = load_progress()\n",
    "        if start_index > 0:\n",
    "            print(f\"Reprise du traitement à partir de l'élément {start_index+1}\")\n",
    "        \n",
    "        # Traiter chaque élément - avec version standard de tqdm\n",
    "        for i, item in enumerate(tqdm(items[start_index:], desc=\"Traitement des résumés\")):\n",
    "            current_index = start_index + i\n",
    "            \n",
    "            try:\n",
    "                if \"summary\" in item and item[\"summary\"]:\n",
    "                    original_summary = item[\"summary\"]\n",
    "                    new_summary = process_summary(original_summary)\n",
    "                    \n",
    "                    if original_summary != new_summary:\n",
    "                        modified_count += 1\n",
    "                        \n",
    "                        if len(original_summary) > len(new_summary):\n",
    "                            removed_words += 1\n",
    "                        else:\n",
    "                            completed_phrases += 1\n",
    "                    \n",
    "                    item[\"summary\"] = new_summary\n",
    "            except Exception as e:\n",
    "                errors.append({\"index\": current_index, \"error\": str(e)})\n",
    "                print(f\"Erreur sur l'élément {current_index}: {e}\")\n",
    "            \n",
    "            # Sauvegarder la progression\n",
    "            save_progress(current_index + 1)\n",
    "            \n",
    "            # Sauvegarde intermédiaire\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                print(f\"\\nSauvegarde intermédiaire après {current_index + 1}/{total_items} éléments...\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    if isinstance(data, list):\n",
    "                        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "                    else:\n",
    "                        json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Écriture finale\n",
    "        print(f\"\\nÉcriture du résultat dans {output_file}...\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Si des erreurs ont été rencontrées, les enregistrer\n",
    "        if errors:\n",
    "            with open(\"errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(errors, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Des erreurs ont été rencontrées pour {len(errors)} éléments. Détails dans errors.json\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraitement terminé en {elapsed_time:.2f} secondes\")\n",
    "        print(f\"Statistiques:\")\n",
    "        print(f\"- Résumés modifiés: {modified_count}/{total_items}\")\n",
    "        print(f\"- Mots uniques supprimés: {removed_words}\")\n",
    "        print(f\"- Phrases complétées: {completed_phrases}\")\n",
    "        \n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Exécution directe\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # Si importé dans un notebook\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# import json\n",
    "# import re\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Configuration\n",
    "# input_file = \"subset.json\"\n",
    "# output_file = \"result.json\"\n",
    "# api_key = \n",
    "# batch_size = 50\n",
    "\n",
    "# # Fonction simple pour compter les mots (sans utiliser NLTK)\n",
    "# def count_words(text):\n",
    "#     \"\"\"Compte les mots dans un texte en utilisant des expressions régulières\"\"\"\n",
    "#     words = re.findall(r'\\b\\w+\\b', text)\n",
    "#     return len(words)\n",
    "\n",
    "# def complete_with_local_model(incomplete_text):\n",
    "#     from transformers import pipeline\n",
    "    \n",
    "#     # Charge un modèle plus léger qui fonctionne sur CPU\n",
    "#     generator = pipeline('text-generation', model='gpt2')\n",
    "#     result = generator(incomplete_text, max_length=30, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "#     # Nettoyer la sortie pour ne garder que la complétion\n",
    "#     return result\n",
    "\n",
    "# def complete_sentence_with_openai(incomplete_text, api_key):\n",
    "#     \"\"\"Complète une phrase incomplète en utilisant l'API OpenAI\"\"\"\n",
    "#     try:\n",
    "#         client = OpenAI(api_key=api_key)\n",
    "        \n",
    "#         response = client.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=f\"Complète cette phrase de manière cohérente et concise: '{incomplete_text}'\",\n",
    "#             max_tokens=30,\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "        \n",
    "#         completed = response.choices[0].text.strip()\n",
    "#         return completed\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de la complétion OpenAI: {e}\")\n",
    "#         # Fallback: retourner le texte avec un point final\n",
    "#         return incomplete_text + \".\"\n",
    "\n",
    "# def process_summary(summary, api_key):\n",
    "#     \"\"\"\n",
    "#     Traite le champ summary selon les règles définies:\n",
    "#     1. Si après le dernier point il n'y a qu'un seul mot, le supprimer\n",
    "#     2. Si après le dernier point il y a plusieurs mots (phrase incomplète), la compléter\n",
    "#     \"\"\"\n",
    "#     # Si le texte est vide ou ne contient pas de point\n",
    "#     if not summary or \".\" not in summary:\n",
    "#         return summary\n",
    "    \n",
    "#     try:\n",
    "#         # Séparer le texte en fonction du dernier point\n",
    "#         main_text, last_part = summary.rsplit(\".\", 1)\n",
    "        \n",
    "#         # Nettoyer la dernière partie\n",
    "#         last_part = last_part.strip()\n",
    "        \n",
    "#         # Si la dernière partie est vide, retourner le texte principal avec un point\n",
    "#         if not last_part:\n",
    "#             return main_text + \".\"\n",
    "        \n",
    "#         # Compter les mots dans la dernière partie - SANS NLTK\n",
    "#         word_count = count_words(last_part)\n",
    "        \n",
    "#         # Si un seul mot, le supprimer\n",
    "#         if word_count == 1:\n",
    "#             print(f\"Suppression du mot '{last_part}' en fin de résumé\")\n",
    "#             return main_text + \".\"\n",
    "        \n",
    "#         # Si plusieurs mots, compléter la phrase avec OpenAI\n",
    "#         elif word_count > 1:\n",
    "#             print(f\"Complétion de la phrase: '{last_part}'\")\n",
    "#             completed_sentence =  complete_with_local_model(last_part)\n",
    "#             return main_text + \". \" + completed_sentence\n",
    "        \n",
    "#         # Cas improbable, mais par sécurité\n",
    "#         else:\n",
    "#             return main_text + \".\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors du traitement du résumé: {e}\")\n",
    "#         return summary  # En cas d'erreur, retourner le résumé original\n",
    "\n",
    "# # Fonction principale\n",
    "# def main():\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     try:\n",
    "#         # Lire le fichier JSON\n",
    "#         print(f\"Lecture du fichier {input_file}...\")\n",
    "#         with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "        \n",
    "#         # Vérifier le format (liste d'objets ou objet unique)\n",
    "#         if isinstance(data, list):\n",
    "#             items = data\n",
    "#         else:\n",
    "#             items = [data]  # Convertir en liste pour traitement uniforme\n",
    "        \n",
    "#         total_items = len(items)\n",
    "#         print(f\"Traitement de {total_items} éléments...\")\n",
    "        \n",
    "#         # Variables pour les statistiques\n",
    "#         modified_count = 0\n",
    "#         removed_words = 0\n",
    "#         completed_phrases = 0\n",
    "        \n",
    "#         # Traiter chaque élément avec une barre de progression\n",
    "#         for i, item in enumerate(tqdm(items, desc=\"Traitement des résumés\")):\n",
    "#             if \"summary\" in item and item[\"summary\"]:\n",
    "#                 original_summary = item[\"summary\"]\n",
    "#                 new_summary = process_summary(original_summary, api_key)\n",
    "                \n",
    "#                 if original_summary != new_summary:\n",
    "#                     modified_count += 1\n",
    "                    \n",
    "#                     # Déterminer si c'était une suppression ou une complétion\n",
    "#                     if len(original_summary) > len(new_summary):\n",
    "#                         removed_words += 1\n",
    "#                     else:\n",
    "#                         completed_phrases += 1\n",
    "                \n",
    "#                 item[\"summary\"] = new_summary\n",
    "            \n",
    "#             # Sauvegarde intermédiaire pour éviter de perdre du travail\n",
    "#             if (i + 1) % batch_size == 0 and i > 0:\n",
    "#                 print(f\"\\nSauvegarde intermédiaire après {i + 1}/{total_items} éléments...\")\n",
    "#                 with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#                     if isinstance(data, list):\n",
    "#                         json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "#                     else:\n",
    "#                         json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "#         # Écrire le résultat final\n",
    "#         print(f\"\\nÉcriture du résultat dans {output_file}...\")\n",
    "#         with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#             if isinstance(data, list):\n",
    "#                 json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "#             else:\n",
    "#                 json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         print(f\"\\nTraitement terminé en {elapsed_time:.2f} secondes\")\n",
    "#         print(f\"Statistiques:\")\n",
    "#         print(f\"- Résumés modifiés: {modified_count}/{total_items}\")\n",
    "#         print(f\"- Mots uniques supprimés: {removed_words}\")\n",
    "#         print(f\"- Phrases complétées: {completed_phrases}\")\n",
    "        \n",
    "#         return 0\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur générale: {e}\")\n",
    "#         return 1\n",
    "\n",
    "# # Exécution directe\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "# else:\n",
    "#     # Si importé dans un notebook, exécuter directement\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pb avec pkuntab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# import json\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Télécharger les ressources NLTK si nécessaire\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "\n",
    "# # Configuration - définissez directement les variables plutôt que d'utiliser argparse\n",
    "# input_file = \"subset.json\"  # Définir directement le fichier d'entrée\n",
    "# output_file = \"result.json\"  # Définir directement le fichier de sortie\n",
    "# batch_size = 50\n",
    "\n",
    "# def complete_sentence_with_openai(incomplete_text, api_key):\n",
    "#     \"\"\"Complète une phrase incomplète en utilisant l'API OpenAI\"\"\"\n",
    "#     try:\n",
    "#         client = OpenAI(api_key=api_key)\n",
    "        \n",
    "#         response = client.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=f\"Complète cette phrase de manière cohérente et concise: '{incomplete_text}'\",\n",
    "#             max_tokens=30,\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "        \n",
    "#         completed = response.choices[0].text.strip()\n",
    "#         return completed\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de la complétion OpenAI: {e}\")\n",
    "#         # Fallback: retourner le texte avec un point final\n",
    "#         return incomplete_text + \".\"\n",
    "\n",
    "# def process_summary(summary, api_key):\n",
    "#     \"\"\"\n",
    "#     Traite le champ summary selon les règles définies:\n",
    "#     1. Si après le dernier point il n'y a qu'un seul mot, le supprimer\n",
    "#     2. Si après le dernier point il y a plusieurs mots (phrase incomplète), la compléter\n",
    "#     \"\"\"\n",
    "#     # Si le texte est vide ou ne contient pas de point\n",
    "#     if not summary or \".\" not in summary:\n",
    "#         return summary\n",
    "    \n",
    "#     # Séparer le texte en fonction du dernier point\n",
    "#     main_text, last_part = summary.rsplit(\".\", 1)\n",
    "    \n",
    "#     # Nettoyer la dernière partie\n",
    "#     last_part = last_part.strip()\n",
    "    \n",
    "#     # Si la dernière partie est vide, retourner le texte principal avec un point\n",
    "#     if not last_part:\n",
    "#         return main_text + \".\"\n",
    "    \n",
    "#     # Compter les mots dans la dernière partie\n",
    "#     words = word_tokenize(last_part)\n",
    "#     word_count = len(words)\n",
    "    \n",
    "#     # Si un seul mot, le supprimer\n",
    "#     if word_count == 1:\n",
    "#         print(f\"Suppression du mot '{last_part}' en fin de résumé\")\n",
    "#         return main_text + \".\"\n",
    "    \n",
    "#     # Si plusieurs mots, compléter la phrase avec OpenAI\n",
    "#     elif word_count > 1:\n",
    "#         print(f\"Complétion de la phrase: '{last_part}'\")\n",
    "#         completed_sentence = complete_sentence_with_openai(last_part, api_key)\n",
    "#         return main_text + \". \" + completed_sentence\n",
    "    \n",
    "#     # Cas improbable, mais par sécurité\n",
    "#     else:\n",
    "#         return main_text + \".\"\n",
    "\n",
    "# # Fonction principale - remplacée par code direct\n",
    "# start_time = time.time()\n",
    "\n",
    "# try:\n",
    "#     # Lire le fichier JSON\n",
    "#     print(f\"Lecture du fichier {input_file}...\")\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     # Vérifier le format (liste d'objets ou objet unique)\n",
    "#     if isinstance(data, list):\n",
    "#         items = data\n",
    "#     else:\n",
    "#         items = [data]  # Convertir en liste pour traitement uniforme\n",
    "    \n",
    "#     total_items = len(items)\n",
    "#     print(f\"Traitement de {total_items} éléments...\")\n",
    "    \n",
    "#     # Variables pour les statistiques\n",
    "#     modified_count = 0\n",
    "#     removed_words = 0\n",
    "#     completed_phrases = 0\n",
    "    \n",
    "#     # Traiter chaque élément avec une barre de progression\n",
    "#     for i, item in enumerate(tqdm(items, desc=\"Traitement des résumés\")):\n",
    "#         if \"summary\" in item and item[\"summary\"]:\n",
    "#             original_summary = item[\"summary\"]\n",
    "#             new_summary = process_summary(original_summary, api_key)\n",
    "            \n",
    "#             if original_summary != new_summary:\n",
    "#                 modified_count += 1\n",
    "                \n",
    "#                 # Déterminer si c'était une suppression ou une complétion\n",
    "#                 if len(original_summary) > len(new_summary):\n",
    "#                     removed_words += 1\n",
    "#                 else:\n",
    "#                     completed_phrases += 1\n",
    "            \n",
    "#             item[\"summary\"] = new_summary\n",
    "        \n",
    "#         # Sauvegarde intermédiaire pour éviter de perdre du travail\n",
    "#         if (i + 1) % batch_size == 0:\n",
    "#             print(f\"\\nSauvegarde intermédiaire après {i + 1}/{total_items} éléments...\")\n",
    "#             with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#                 if isinstance(data, list):\n",
    "#                     json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "#                 else:\n",
    "#                     json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "#     # Écrire le résultat final\n",
    "#     print(f\"\\nÉcriture du résultat dans {output_file}...\")\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         if isinstance(data, list):\n",
    "#             json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "#         else:\n",
    "#             json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     print(f\"\\nTraitement terminé en {elapsed_time:.2f} secondes\")\n",
    "#     print(f\"Statistiques:\")\n",
    "#     print(f\"- Résumés modifiés: {modified_count}/{total_items}\")\n",
    "#     print(f\"- Mots uniques supprimés: {removed_words}\")\n",
    "#     print(f\"- Phrases complétées: {completed_phrases}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Juste enlever les phrases incomplètes du dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture du fichier medical_summaries_tronqués.json...\n",
      "Traitement de 5000 éléments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage des résumés: 100%|██████████| 5000/5000 [00:00<00:00, 1100174.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écriture du résultat dans result_clean_5000.json...\n",
      "\n",
      "Traitement terminé en 0.37 secondes\n",
      "Résumés modifiés: 4610/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "input_file = \"medical_summaries_tronqués.json\"\n",
    "output_file = \"result_clean_5000.json\"\n",
    "\n",
    "def clean_summary(summary):\n",
    "    \"\"\"Supprime tout ce qui suit le dernier point dans un texte\"\"\"\n",
    "    if not summary or \".\" not in summary:\n",
    "        return summary\n",
    "    \n",
    "    # Diviser au dernier point et ne garder que la partie avant\n",
    "    main_text, _ = summary.rsplit(\".\", 1)\n",
    "    \n",
    "    # Retourner le texte principal avec un point\n",
    "    return main_text + \".\"\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Lire le fichier JSON\n",
    "        print(f\"Lecture du fichier {input_file}...\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Vérifier le format (liste ou objet unique)\n",
    "        if isinstance(data, list):\n",
    "            items = data\n",
    "        else:\n",
    "            items = [data]\n",
    "        \n",
    "        total_items = len(items)\n",
    "        print(f\"Traitement de {total_items} éléments...\")\n",
    "        \n",
    "        # Statistiques\n",
    "        modified_count = 0\n",
    "        \n",
    "        # Traiter chaque élément\n",
    "        for item in tqdm(items, desc=\"Nettoyage des résumés\"):\n",
    "            if \"summary\" in item and item[\"summary\"]:\n",
    "                original_summary = item[\"summary\"]\n",
    "                new_summary = clean_summary(original_summary)\n",
    "                \n",
    "                if original_summary != new_summary:\n",
    "                    modified_count += 1\n",
    "                \n",
    "                item[\"summary\"] = new_summary\n",
    "        \n",
    "        # Écrire le résultat\n",
    "        print(f\"Écriture du résultat dans {output_file}...\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            if isinstance(data, list):\n",
    "                json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                json.dump(items[0], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraitement terminé en {elapsed_time:.2f} secondes\")\n",
    "        print(f\"Résumés modifiés: {modified_count}/{total_items}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Etape 3 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données depuis result_clean_5000.json...\n",
      "Jeu de données chargé: 5000 documents\n",
      "Mélange aléatoire des données...\n",
      "Division effectuée:\n",
      "- Ensemble d'entraînement: 3500 documents (70.0%)\n",
      "- Ensemble de validation: 500 documents (10.0%)\n",
      "- Ensemble de test: 1000 documents (20.0%)\n",
      "Sauvegarde des ensembles...\n",
      "Création de versions CSV des ensembles...\n",
      "\n",
      "Division des données terminée avec succès!\n",
      "Les fichiers ont été sauvegardés dans le répertoire 'dataset_splits'\n",
      "\n",
      "Statistiques des ensembles:\n",
      "Train - Longueur moyenne des résumés: 94.7 tokens\n",
      "Train - Résumé min/max: 0/1073 tokens\n",
      "Validation - Longueur moyenne des résumés: 96.8 tokens\n",
      "Validation - Résumé min/max: 0/678 tokens\n",
      "Test - Longueur moyenne des résumés: 94.7 tokens\n",
      "Test - Résumé min/max: 0/497 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "input_file = \"result_clean_5000.json\"\n",
    "output_dir = \"dataset_splits\"\n",
    "train_ratio = 0.7  # Corrigé à 70%\n",
    "val_ratio = 0.1    # 10%\n",
    "test_ratio = 0.2   # 20%\n",
    "\n",
    "def main():\n",
    "    # Créer le répertoire de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Charger les données\n",
    "    print(f\"Chargement des données depuis {input_file}...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # S'assurer que les données sont dans une liste\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    total_items = len(data)\n",
    "    print(f\"Jeu de données chargé: {total_items} documents\")\n",
    "    \n",
    "    # Mélanger les données pour éviter tout biais lié à l'ordre\n",
    "    print(\"Mélange aléatoire des données...\")\n",
    "    random.seed(42)  # Pour la reproductibilité\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculer les tailles des ensembles\n",
    "    train_size = int(total_items * train_ratio)\n",
    "    val_size = int(total_items * val_ratio)\n",
    "    test_size = total_items - train_size - val_size\n",
    "    \n",
    "    # Diviser les données\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Division effectuée:\")\n",
    "    print(f\"- Ensemble d'entraînement: {len(train_data)} documents ({train_ratio*100:.1f}%)\")\n",
    "    print(f\"- Ensemble de validation: {len(val_data)} documents ({val_ratio*100:.1f}%)\")\n",
    "    print(f\"- Ensemble de test: {len(test_data)} documents ({len(test_data)/total_items*100:.1f}%)\")\n",
    "    \n",
    "    # Sauvegarder les ensembles\n",
    "    print(\"Sauvegarde des ensembles...\")\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"train.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"validation.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"test.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Créer également des versions CSV pour plus de flexibilité\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        print(\"Création de versions CSV des ensembles...\")\n",
    "        \n",
    "        # Fonction pour convertir en DataFrame\n",
    "        def convert_to_df(data_list):\n",
    "            # Adaptation selon la structure de vos données\n",
    "            # Supposant que chaque élément a un champ 'text' et 'summary'\n",
    "            rows = []\n",
    "            for item in data_list:\n",
    "                row = {}\n",
    "                # Ajoutez ici tous les champs que vous voulez conserver\n",
    "                if 'text' in item:\n",
    "                    row['text'] = item['text']\n",
    "                if 'summary' in item:\n",
    "                    row['summary'] = item['summary']\n",
    "                # Ajoutez d'autres champs au besoin\n",
    "                rows.append(row)\n",
    "            return pd.DataFrame(rows)\n",
    "        \n",
    "        # Convertir et sauvegarder\n",
    "        convert_to_df(train_data).to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
    "        convert_to_df(val_data).to_csv(os.path.join(output_dir, \"validation.csv\"), index=False)\n",
    "        convert_to_df(test_data).to_csv(os.path.join(output_dir, \"test.csv\"), index=False)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Module pandas non disponible. Création des fichiers CSV ignorée.\")\n",
    "    \n",
    "    print(\"\\nDivision des données terminée avec succès!\")\n",
    "    print(f\"Les fichiers ont été sauvegardés dans le répertoire '{output_dir}'\")\n",
    "    \n",
    "    # Statistiques optionnelles\n",
    "    print(\"\\nStatistiques des ensembles:\")\n",
    "    \n",
    "    def count_tokens(text):\n",
    "        # Méthode simple pour estimer le nombre de tokens\n",
    "        return len(text.split())\n",
    "    \n",
    "    for split_name, dataset in [(\"Train\", train_data), (\"Validation\", val_data), (\"Test\", test_data)]:\n",
    "        try:\n",
    "            # Calculer des statistiques sur les longueurs de texte et de résumé\n",
    "            text_lengths = [count_tokens(item.get('text', '')) for item in dataset if 'text' in item]\n",
    "            summary_lengths = [count_tokens(item.get('summary', '')) for item in dataset if 'summary' in item]\n",
    "            \n",
    "            if text_lengths:\n",
    "                print(f\"{split_name} - Longueur moyenne des textes: {sum(text_lengths)/len(text_lengths):.1f} tokens\")\n",
    "                print(f\"{split_name} - Texte min/max: {min(text_lengths)}/{max(text_lengths)} tokens\")\n",
    "            if summary_lengths:\n",
    "                print(f\"{split_name} - Longueur moyenne des résumés: {sum(summary_lengths)/len(summary_lengths):.1f} tokens\")\n",
    "                print(f\"{split_name} - Résumé min/max: {min(summary_lengths)}/{max(summary_lengths)} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Impossible de calculer les statistiques pour l'ensemble {split_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
